---
title: "AMProject_Clean"
author: "Laura Gullicksen, Erich Gozebina, Daria Palitzsch"
date: "23/05/2025"
output:
  pdf_document:
    number_sections: false
  html_document:
    df_print: paged
  word_document: default
fontsize: 12pt
geometry: margin=1in
header-includes:
- \usepackage{titlesec}
- \titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
- \titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}
- \titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}
- \usepackage{float}
- \usepackage{placeins}
- \FloatBarrier
- \usepackage{mdframed}
---

```{r setup, include=FALSE}
#setup chunk to load all required packages

knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(ggplot2)
library(skimr)
library(dplyr)
library(slider)
library(DescTools)
library(stargazer)
# library(ggpubr)
# #library(ggalluvial)
library(lubridate)
# library(benford.analysis)
# library(tidyquant)
# library(plotly)
# library(RcppRoll)
# library(scales)
# library(cowplot)
# library(ggforce)
# library(circlize)
library(zoo)
library(glmnet)
# library(PerformanceAnalytics)
# library(VGAM)
# library(DescTools)
# library(readxl)
# library(broom)
# library(Quandl)
library(kableExtra)
# library(flextable)
# library(gridExtra)
# library(tinytex)
library(TTR)



## setting the correct time zone
Sys.setlocale("LC_TIME", "English") #set language for dates etc.
Sys.setenv(TZ='UCT') #set timezone!

```


## 1. Introduction

#TODO: describe data choice -> Laura 

## 2. Data & Descriptive Analysis

**Data Aggregation and Strategy Frequency**

The raw dataset provides CHAINLINK price data at \textit{hourly frequency}. While such high-frequency data offers more granular insights, we chose to \textbf{aggregate the data to daily frequency} for the following reasons: 

1. **Alignment with Trading Strategy**: Our core trading strategy is based on a **7-day momentum signal**, which inherently reflects **weekly price trends**. Applying such a signal at an hourly resolution would not be consistent with the strategy's time horizon. 

2. **Noise Reduction**: Hourly crypto data can be highly volatile and noisy. Aggregating to daily returns reduces **microstructure noise, short-term reversals**, and **Whale-driven price spikes**, improving the signal-to-noise ratio. 

3. **Practical Execution Perspective**: A strategy that rebalances daily is **more realistic to implement**, considering gas fees, latencyn and operational constraints on decentralized exchanges or CEX APIs. 

4. **Interpretability and Robustness**: Daily returns are more interpretable and robust across backtests. Most financial and technical indicators (e.g., RSI, MACD, SMA) are commonly applied on daily charts. 

Our strategy issues long/short signals based on the past 7-day log return of CHAINLINK, i.e., 

\[
\text{Momentum}_t^{(7)} = \log\left(\frac{P_t}{P_{t-7}}\right)
\]

This naturally assumes daily data, as each observation reflects the cumulative return over the previous seven days.

In summary, aggregating to daily frequency is a theoretically and practically sound choice. It ensures consistency between our signal construction, model estimation, and backtesting logic.


```{r, echo = FALSE}
#Load the price data of CHAINLINK
prices_link <- read.csv("data/pricedata/hourly_prices_0x514910771AF9Ca656af840dff83E8264EcF986CA.csv")

#change the data type of the timestamp
prices_link <- prices_link %>% 
  mutate(hour_timestamp = as.POSIXct(hour_timestamp, format = "%Y-%m-%d %H:%M")) %>%
  # mutate(hour_timestamp = as.Date(ymd_hms(hour_timestamp))) %>% 
  arrange(hour_timestamp) # sort from old to new


# Aggregate to daily close price (last available hourly close of each day)
df_daily <- prices_link %>%
  mutate(date = as.Date(hour_timestamp)) %>%
  group_by(date) %>%
  summarise(
    close_price = last(close_price),
    open_price = first(open_price),
    high_price = max(high_price),
    low_price = min(low_price)
  ) %>%
  arrange(date) %>%
  mutate(
    log_return = log(close_price / lag(close_price)),
    log_open_return = log(open_price / lag(open_price)),
    log_high_return = log(high_price / lag(high_price)),
    log_low_return = log(low_price / lag(low_price)) 
  )

# View the result
#head(df_daily)

```


```{r, echo=FALSE}
# Plot prices
ggplot(df_daily) + aes(x=date, y=close_price) + 
geom_line(color = "steelblue") + 
labs(title = "Daily LINK Close Price in USD", 
     x = "Date", 
     y = "Close Price (USD)"
     )

# Plot log returns
# ggplot(df_daily%>%drop_na()) + aes(x=date, y=log_return) +
# geom_point(color = "darkred") +
# labs(title = "Daily LINK Log Returns", xlab = "Date", ylab = "Log Return")

ggplot(df_daily, aes(x = date, y = log_return)) +
  geom_point(color = "#990000", size = 1.5, alpha = 0.7) +  # smaller, translucent points
  labs(
    title = "Daily Log Returns of Chainlink",
    subtitle = "Time series plot from 2017 to 2024",
    x = "Date",
    y = "Log Return"
  ) +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(margin = margin(b = 10)),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.minor = element_blank()
  )
```

#DONE: Explain output and log choice -> Erich

Having a first glance on the LINK's price data, we see that there is not much movement until 2020, followed by sharp increases by factor ten in the consecutive nine months. The price reaches its peak of over 50\$/LINK in May 2021. In the second half of 2021 until April 2022 the data shows volatile behavior but with significant decrease on average. A trendless period of relatively low volatility, starting in May 2022 and ending in October 2023, shows prices between 5 and 10\$/LINK. Finally, we observe another increase at the end of 2023 and beginning of 2024.

Moving away from non-stationary price data to stationary log returns, we observe a higher dispersion in the earlier years, suggesting a higher volatility then. Furthermore, most of points cluster around zero, indicating that there is no significant long term drift. Outliers, both positive and negative, imply extreme relative price movements, especially in the earlier period. Another important insight is the changing variance since the density of the points increases in the second half of the time window.

Why did we chose log returns over canonical (arithmetic) returns? Using log returns instead of canonical returns is a standard practice in financial econometrics and modeling. 

\[
\tilde{r}_t = \log(r_t + 1) = \log\left(\frac{p_t}{p_{t-1}}\right)
\]

The underlying reason is the assumption that prices of an financial asset are log-normally distributed. This is reasonable since the log-normal distribution does not allow for negative values, which is also true for most asset prices (particularly for crypto currencies). Moreover, historical data provides evidence that the log-normal distribution gives a good fit for the prices of many financial assets. In reverse, since the logarithm function amplifies returns that are close to -1 more than positive returns, log-returns are distributed more symmetrically than canonical returns and indeed follow a normal distribution. Additionally, if returns are small, log returns approximate canonical returns very well. For x close to zero, it holds that

\[
\log(x+1) \approx x.
\]

We can expect small returns since we shorten the considered time interval. Another important property is the additivity of log returns. It allows us to aggregate returns over multiple periods by summing up the pointwise log returns - a property that canonical returns miss. These properties make log returns more suitable for linear regression models, hypothesis testing, and machine learning regressors.



To better understand the characteristics of the Chainlink price and return series, we compute a set of descriptive statistics based on the daily close prices and the corresponding log returns. These statistics provide a first impression of the dataset’s distribution, dispersion, and extreme values, and help assess whether further preprocessing or transformation steps are necessary before applying predictive models.


```{r, echo = FALSE}
# Descriptive stats for prices and returns
summary_stats <- df_daily %>%
  summarise(
    n_obs = n(),
    mean_close = mean(close_price, na.rm = TRUE),
    sd_close = sd(close_price, na.rm = TRUE),
    min_close = min(close_price, na.rm = TRUE),
    max_close = max(close_price, na.rm = TRUE),
    mean_return = mean(log_return, na.rm = TRUE),
    sd_return = sd(log_return, na.rm = TRUE),
    min_return = min(log_return, na.rm = TRUE),
    max_return = max(log_return, na.rm = TRUE)
  )

summary_stats_long <- as.data.frame(t(summary_stats))
colnames(summary_stats_long) <- "Value"

# Add a column for metric names
summary_stats_long <- tibble::rownames_to_column(summary_stats_long, var = "Statistic")

#Format values: no scientific notation and rounded to 4 digits
summary_stas_long_formatted <- summary_stats_long %>% 
  mutate(Value = formatC(Value, format = "f", digits = 4, big.mark = ",")) %>%
  mutate(Statistic = dplyr::recode(Statistic,
    n_obs        = "Number of Observations",
    mean_close   = "Mean Close Price",
    sd_close     = "Std. Dev. Close Price",
    min_close    = "Minimum Close Price",
    max_close    = "Maximum Close Price",
    mean_return  = "Mean Return",
    sd_return    = "Std. Dev. Return",
    min_return   = "Minimum Return",
    max_return   = "Maximum Return"
  ))

summary_stas_long_formatted %>%
  kable("latex", booktabs = TRUE, aling = "lr", caption = "Summary Statistics for Chainlink Price and Returns") %>% 
  kable_styling(latex_options = c("hold_position", "striped"))

```

The summary statistics reveal that the mean daily log return of Chainlink is close to zero, while the standard deviation is relatively high, reflecting the well-known volatility of cryptocurrency markets. The minimum and maximum returns further highlight the presence of large price swings. The wide range between the minimum and maximum close prices illustrates the strong appreciation potential, but also the riskiness of the asset over the observation period.


To evaluate the temporal dependence structure of Chainlink’s daily log returns, we plot the autocorrelation function (ACF). The ACF helps determine whether past returns exhibit statistically significant correlation with future returns — a key consideration when assessing the potential for return predictability.

```{r acf_plot, fig.cap = "ACF of Daily Log Returns for Chainlink", fig.align = "center", echo = FALSE, message = FALSE, warning = FALSE}
# Plot ACF of daily log returns
acf(
  na.omit(df_daily$log_return),
  main = "Autocorrelation of Daily Log Returns (LINK)",
  col = "#1f78b4",      # blue line
  lwd = 2,              # thicker line
  cex.lab = 1.2,        # label size
  cex.main = 1.3        # title size
)
```

The autocorrelation function (ACF) of daily log returns shows no statistically significant linear dependence at any lag, indicating that past returns do not linearly predict future returns. This finding supports the weak-form Efficient Market Hypothesis (EMH). However, it does not rule out the presence of exploitable patterns captured by non-linear or directional indicators. Therefore, we proceed with a momentum-based trading strategy, leveraging the sign of multi-day past returns to generate long or short signals.


## 3. Standard Model 

We develop a basic model that will serve as a starting point for an extended model. Our first approach to predict future returns is a simple linear regression. Why linear regression? First, this technique is the underlying mechanism of many advanced models that are often generalizations of the linear case. Therefore, it is a good fit for a starting point. In general, linear regression aims to identify linear relationships between input data and the target dimension. In our case the input data is price data, trading volume, market capitalization, and every predictor that is derived from those - returns for instance. The target dimension that we are going to predict is the return of the next day. The simplicity of linear mappings make results easy to interpret, whereby the model still remains powerful since many observed relationships are indeed of linear nature. Moreover, linear regression indicates the strength of those linear ties what makes it a helpful tool for decision making.

**7-Day Momentum Signal Strategy**

We define the 7-day momentum as the log return over the past 7 days:
\[
\text{Momentum}_t = \log\left(\frac{P_t}{P_{t-7}}\right)
\]

The trading signal is then determined as:
\[
\text{Signal}_t =
\begin{cases}
+1 & \text{if } \text{Momentum}_t > 0 \quad \text{(go long)} \\
-1 & \text{if } \text{Momentum}_t < 0 \quad \text{(go short)} \\
\;\;0 & \text{otherwise (no position)}
\end{cases}
\]

The strategy return is computed as:
\[
r^{\text{strategy}}_{t+1} = \text{Signal}_t \cdot r_{t+1}
\]
where \( r_{t+1} = \log\left(\frac{P_{t+1}}{P_t}\right) \) is the daily log return.

```{r, echo = FALSE}

# 1. Compute 7-day momentum
df_daily <- df_daily %>%
  mutate(
    momentum_7d = log(close_price / lag(close_price, 7)),
    signal = case_when(
      momentum_7d > 0 ~ 1,   # Long
      momentum_7d < 0 ~ -1,  # Short
      TRUE ~ 0               # No signal
    )
  )

# 2. Shift signal forward by one day to avoid look-ahead bias
df_daily <- df_daily %>%
  mutate(
    signal_lagged = lag(signal),
    strategy_return = signal_lagged * log_return
  )

```


#TODO: insert standard model with momentum -> Erich
```{r results='asis', echo = FALSE}
#standard model with 7-day-momentum
model_standard <- lm(strategy_return ~ momentum_7d, data = df_daily)

summary(model_standard)

stargazer(model_standard,
          type = "latex",
          title = "Regression Results: 7-Day Momentum Strategy",
          label = "tab:momentum_model",
          style = "default",
          dep.var.labels = "Strategy Return",
          covariate.labels = c("Intercept", "7-Day Momentum"),
          digits = 4,
          float.env = "table",
          header = FALSE)
```


#TODO: explain result of standard 7 day momentum strategy -> Laura 
# Draft from Erich:
The linear regression results in an estimation for the intercept of 0.002 having a p-value of 0.0999 and a slope of 0.005 having a p-value of 0.5123. Since the p-value for the slope clearly exceeds 0.05 (the threshold, which is commonly used for acceptance), we can not conclude a linear relation between the 7-day momentum and the future day return. Actually, this aligns with our finding in the ACF analysis, where no linear dependency between a lag of 7 days and the future day return was indicated. Thus, this result calls for an extended approach of future return prediction.


## 4. Extension 

**Extension of our OLS**

To enhance the predictive power of the benchmark model, we extend it by incorporating a broader set of explanatory variables that capture not only short- and medium-term price dynamics, but also market sentiment, technical indicators, and inter-asset relationships. These include:

\begin{itemize}
  \item Momentum indicators over 3, 7, and 14 days,
  \item Lagged daily returns (1-day and 2-day),
  \item A 7-day rolling volatility measure,
  \item Technical indicators such as the 14-day Relative Strength Index (RSI), MACD value and histogram, Simple Moving Average difference, and Average True Range (ATR),
  \item Day-of-week dummy variables to capture potential calendar effects,
  \item BTC-based predictors: daily BTC return, 7-day BTC momentum, and 7-day BTC volatility,
  \item ETH-based predictors: daily ETH return, 7-day ETH momentum, and 7-day ETH volatility,
  \item ETH trading volume: daily ETH volume return, 7-day ETH volume momentum, and 7-day ETH volume volatility,
  \item ETH market capitalization: daily ETH market capitalization return, 7-day ETH market capitalization momentum, and 7-day ETH market capitalization volatility,
  \item Ethereum gas fees: daily gas return, 7-day gas momentum, and 7-day gas volatility.
\end{itemize}

The extended predictive regression model is specified as:

\[
r_{t+1} = \alpha + \sum_{h \in \{3,7,14\}} \beta_h \cdot \text{Momentum}_t^{(h)} + \gamma_1 \cdot r_t + \gamma_2 \cdot r_{t-1} + \delta \cdot \text{Volatility}_t^{(7)} + \sum_j \theta_j \cdot X_{t}^{(j)} + \varepsilon_{t+1}
\]

where \( X_t^{(j)} \) represents the set of technical indicators (RSI, MACD, ATR, SMA), weekday dummies, and BTC-based predictors.

\begin{align*}
r_{t+1} &:= \log\left(\frac{P_{t+1}}{P_t}\right) \quad \text{(one-day-ahead LINK return)} \\
\text{Momentum}_t^{(h)} &:= \log\left(\frac{P_t}{P_{t-h}}\right) \quad \text{for } h \in \{3, 7, 14\} \\
\text{Volatility}_t^{(7)} &:= \text{std} \left( r_{t-6}, \ldots, r_t \right) \\
\text{BTC return}_t &:= \log\left(\frac{P^{\text{BTC}}_t}{P^{\text{BTC}}_{t-1}} \right) \\
\text{BTC Momentum}_t^{(7)} &:= \log\left(\frac{P^{\text{BTC}}_t}{P^{\text{BTC}}_{t-7}}\right) \\
\text{BTC Volatility}_t^{(7)} &:= \text{std} \left( r^{\text{BTC}}_{t-6}, \ldots, r^{\text{BTC}}_t \right)
\end{align*}
The ETH-based predictors are constructed analogously to the BTC-based predictors.
The parametrization of this model is estimated via Ordinary Least Squares (OLS) on the in-sample period. By incorporating this rich feature set, we aim to capture a range of return drivers including price trends, market overreaction, volatility clustering, inter-market dependencies, and behavioral biases tied to trading weekdays.


```{r, echo = FALSE}
#We load the bitcoin data on our own because we want to have the full time period aligned with the Chainlink data we have

# Source https://coinmarketcap.com/currencies/bitcoin/historical-data/
own_btc_prices <- read.csv("data/pricedata/Daily_Bitcoin_Own.csv", sep = ";") 
# Source https://coinmarketcap.com/currencies/ethereum/historical-data/
own_eth_prices <- read.csv("data/pricedata/ETH_pricedata.csv", sep = ";")
# Source https://etherscan.io/chart/gasprice
own_eth_gas <- read.csv("data/pricedata/ETH_daily_gas.csv", sep = ",")


own_btc_prices <- own_btc_prices %>% 
  mutate(date = as.Date(timestamp)) %>%
  mutate(
    btc_return = log(close / lag(close)),
    btc_momentum_7d = log(close / lag(close, 7)),
    btc_volatility_7d = rollapply(log(close / lag(close)),
                                  width = 7, FUN = sd, fill = NA, align = "right")
  ) %>%
  select(date, btc_return, btc_momentum_7d, btc_volatility_7d)

own_eth_prices <- own_eth_prices %>%
  mutate(date = as.POSIXct(timeClose, format = "%Y-%m-%d")) %>%
  select(date, open, high, low, close, volume, marketCap) %>%
  arrange(date) # sort from old to new

own_eth_gas <- own_eth_gas %>% 
  rename(date = Date.UTC., gas_wei = Value..Wei.)%>%
  mutate(date = as.POSIXct(date, format = "%m/%d/%Y")) %>%
  select(date, gas_wei) %>%
  arrange(date) # sort from old to new

eth_data <- own_eth_prices %>%  
  left_join(own_eth_gas, by = "date") %>%
  mutate(
    fees_dollar <- gas_wei*10^-18*60000*close, #factor 60,000 for ERC20 token transfer
    eth_return = log(close / lag(close)),
    eth_momentum_7d = log(close / lag(close, 7)),
    eth_volatility_7d = rollapply(log(close / lag(close)),
                                  width = 7, FUN = sd, fill = NA, align = "right"),
    eth_return_volume = log(volume / lag(volume)),
    eth_momentum_7d_volume = log(volume / lag(volume, 7)),
    eth_volatility_7d_volume = rollapply(log(volume / lag(volume)),
                                  width = 7, FUN = sd, fill = NA, align = "right"),
    eth_return_marketcap = log(marketCap / lag(marketCap)),
    eth_momentum_7d_marketcap = log(marketCap / lag(marketCap, 7)),
    eth_volatility_7d_marketcap = rollapply(log(marketCap / lag(marketCap)),
                                  width = 7, FUN = sd, fill = NA, align = "right"),
    eth_return_gas = log(gas_wei / lag(gas_wei)),
    eth_momentum_7d_gas = log(gas_wei / lag(gas_wei, 7)),
    eth_volatility_7d_gas = rollapply(log(gas_wei / lag(gas_wei)),
                                  width = 7, FUN = sd, fill = NA, align = "right")
  )

df_daily <- df_daily %>%  
  left_join(own_btc_prices, by = "date")  %>%
  left_join(eth_data, by = "date")
  

  
  
```



#DONE: add ethereum data -> Erich


```{r, echo = FALSE}
# Step 1: Add features to the dataset
df_extended <- df_daily %>%
  mutate(
    # Momentum over 3, 7 and 14 days
    momentum_3d = log(close_price / lag(close_price, 3)),
    momentum_7d = log(close_price / lag(close_price, 7)),
    momentum_14d = log(close_price / lag(close_price, 14)),

    # Lagged daily returns
    return_lag1 = lag(log_return, 1),
    return_lag2 = lag(log_return, 2),

    # Volatility: rolling 7-day standard deviation of returns
    volatility_7d = rollapply(log_return, width = 7, FUN = sd, align = "right", fill = NA),
    
    # RSI over 14 days
    rsi_14 = RSI(close_price, n = 14),
    
    # Moving average trend signals
    sma_7 = SMA(close_price, n = 7),
    sma_14 = SMA(close_price, n = 14),
    
    # difference between short and long Moving average (trend strength)
    sma_diff = sma_7 - sma_14,
    
    #MACD
    macd_val = data.frame(MACD(close_price, nFast = 12, nSlow = 26, nSig = 9))$macd,
    macd_signal = data.frame(MACD(close_price, nFast = 12, nSlow = 26, nSig = 9))$signal,
    macd_hist = macd_val - macd_signal,
    
    #ATR (Average True Range) over 14 days
    atr_14 = data.frame(ATR(HLC = data.frame(
      high = high_price,
      low = low_price,
      close = close_price), n = 14))$atr,
    
    #weekdays
    weekday = wday(date, label = TRUE, abbr = TRUE),
         monday = ifelse(weekday == "Mon", 1, 0),
         tuesday = ifelse(weekday == "Tue", 1, 0),
         wednesday = ifelse(weekday == "Wed", 1, 0),
         thursday = ifelse(weekday == "Thu", 1, 0),
         friday = ifelse(weekday == "Fri", 1, 0),

    # Target: next day's return
    target_return = lead(log_return, 1)
  ) %>%
  drop_na()  # Remove rows with missing values

# Step 2: Fit the extended linear model
model_extended <- lm(target_return ~ 
                       momentum_3d + momentum_7d + momentum_14d + 
                            return_lag1 + return_lag2 + 
                            volatility_7d + rsi_14 + sma_diff + 
                            macd_val + macd_hist + atr_14 + 
                            monday + tuesday + wednesday + thursday + friday +
                            btc_return + btc_momentum_7d + btc_volatility_7d +
                            eth_return + eth_momentum_7d + eth_volatility_7d +
                            eth_return_volume + eth_momentum_7d_volume + 
                            eth_volatility_7d_volume + 
                            eth_return_marketcap + eth_momentum_7d_marketcap + 
                            eth_volatility_7d_marketcap + 
                            eth_return_gas + eth_momentum_7d_gas + 
                            eth_volatility_7d_gas,
                     data = df_extended)

# Step 3: Summary of model results
summary(model_extended)

```

```{r results='asis', echo=FALSE}
stargazer(model_extended,
          type = "latex",
          title = "Extended Regression Model: Predicting LINK Returns with Crypto Features",
          label = "tab:extended_model",
          dep.var.labels = "Target Return",
          covariate.labels = c("Intercept",
                               "Momentum (3d)", "Momentum (7d)", "Momentum (14d)",
                               "Lagged Return (1d)", "Lagged Return (2d)", "Volatility (7d)",
                               "RSI (14)", "SMA Diff", "MACD Value", "MACD Histogram", "ATR (14)",
                               "Monday", "Tuesday", "Wednesday", "Thursday", "Friday",
                               "BTC Return", "BTC Momentum (7d)", "BTC Volatility (7d)",
                               "ETH Return", "ETH Momentum (7d)", "ETH Volatility (7d)",
                               "ETH Volume", "ETH Momentum (7d) x Volume", "ETH Volatility (7d) x Volume",
                               "ETH Market Cap", "ETH Momentum (7d) x Market Cap", "ETH Volatility (7d) x Market Cap",
                               "ETH Gas", "ETH Momentum (7d) x Gas", "ETH Volatility (7d) x Gas"),
          digits = 4,
          float.env = "table",
          header = FALSE)
```



#TODO: generate nicer latex table output of the regression results -> Daria


#TODO: Description and interpretation of output -> Laura 



**Lasso Model**

To prevent overfitting and perform automatic variable selection, we extend our linear modeling approach using the Lasso (Least Absolute Shrinkage and Selection Operator). The Lasso adds a penalty term to the standard OLS loss function, shrinking some coefficient estimates toward zero. This results in a sparse model that may improve predictive performance, particularly when dealing with multiple correlated predictors. Furthermore, reducing the number of relevant features allows for a better interpretation of simulation results.

The Lasso estimator is defined as the solution to the following optimization problem:

\[
\hat{\beta}^{\text{lasso}} = \arg \min_{\beta_0, \beta} \left\{ \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j \right)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\}
\]

where:
\begin{itemize}
  \item \( y_i \) is the target variable (e.g., one-day-ahead return),
  \item \( x_{ij} \) are the predictor variables,
  \item \( \beta_j \) are the coefficients,
  \item \( \lambda \geq 0 \) is the tuning parameter controlling the strength of the penalty.
\end{itemize}

As \( \lambda \) increases, more coefficients are shrunk toward zero. For \( \lambda = 0 \), the solution coincides with OLS.

We use 10-fold cross-validation to select the optimal \( \lambda \) that minimizes the mean squared prediction error on held-out data.

```{r echo = FALSE, results = 'asis'}

# Step 1: Create the design matrix and response
X <- model.matrix(target_return ~ 
                    momentum_3d + momentum_7d + momentum_14d + 
                    return_lag1 + return_lag2 + 
                    volatility_7d + rsi_14 + sma_diff + 
                    macd_val + macd_hist + atr_14 + 
                    monday + tuesday + wednesday + thursday + friday +
                    btc_return + btc_momentum_7d + btc_volatility_7d +
                    eth_return_volume + eth_momentum_7d_volume + 
                    eth_volatility_7d_volume +
                    eth_return_marketcap + eth_momentum_7d_marketcap + 
                    eth_volatility_7d_marketcap +
                    eth_return_gas + eth_momentum_7d_gas + eth_volatility_7d_gas,
                  data = df_extended)[, -1]  # remove intercept column
y <- df_extended$target_return

# Step 2: Run cross-validated Lasso
set.seed(42)
cv_lasso <- cv.glmnet(X, y, alpha = 1, nfolds = 10)

# Plot cross-validation curve
#plot(cv_lasso)

# # Best lambda
# best_lambda <- cv_lasso$lambda.min
# #cat("Optimal lambda:", best_lambda, "\n")
# cat(sprintf("**Optimal Lambda from Cross-Validation:** $\\lambda^* = %.6f$", best_lambda))

# Explanation for lambda
#cat("The **optimal lambda** ($\\lambda^*$) is a key regularization parameter in LASSO regression. It controls the strength of the penalty applied to the model's coefficients. A higher value of $\\lambda$ increases shrinkage, setting more coefficients to zero and reducing model complexity. The optimal value is chosen via cross-validation to balance model fit and generalization.\n\n")

# Best lambda output
best_lambda <- cv_lasso$lambda.min
cat(sprintf("**Optimal Lambda from Cross-Validation:** $\\lambda^* = %.6f$\n\n", best_lambda))
```

Given this optimal lambda, we now run the LASSO regression, including all variables from the previous OLS regression:

```{r, echo = FALSE}


#Fit final Lasso model using best lambda
model_lasso <- glmnet(X, y, alpha = 1, lambda = best_lambda)

# Show coefficients
coef(model_lasso)


coef_df <- as.matrix(coef(model_lasso))
non_zero <- coef_df[coef_df != 0, , drop = FALSE]

lasso_table <- data.frame(
  Predictor = rownames(non_zero),
  Coefficient = round(non_zero[, 1], 6),
  row.names = NULL
)

nice_names <- c(
  "(Intercept)" = "Intercept",
  "btc_return" = "Bitcoin Daily Return",
  "eth_momentum_7d_volume" = "Ethereum 7-Day Volume Momentum"
)

# Apply renaming
lasso_table$Predictor <- dplyr::recode(lasso_table$Predictor, !!!nice_names)


kable(lasso_table, format = "latex", booktabs = TRUE, 
      caption = "Non-Zero Coefficients from LASSO Regression",
      label = "tab:lasso_coeffs",
      align = c("l", "r")) %>%
  kable_styling(latex_options = c("striped", "hold_position"))

```

The LASSO regression identified two non-zero predictors for explaining Chainlink (LINK) returns:

1. Bitcoin Daily Return (Coefficient: -0.9374): This variable has a large and negative coefficient, indicating that when Bitcoin’s daily return increases by 1 unit (in our scaled units), the predicted return of our LINK-based trading strategy decreases by approximately 0.9374 units, all else equal. This suggests a strong inverse relationship between BTC movements and our strategy, potentially due to hedging behavior or negative spillovers.

2. Ethereum 7-Day Volume Momentum (Coefficient: 0.0008): This predictor captures short-term trends in Ethereum’s trading volume. The positive but small coefficient implies that higher recent momentum in ETH trading volume is weakly associated with increased LINK returns, possibly due to spillover effects from rising market activity in related tokens.

3. Intercept (Coefficient: 0.0011): The intercept represents the model’s baseline prediction when all predictors are zero. Here, it suggests a small positive base return, though in practice this often has less interpretive value than the covariates.


## 5. Forecasting & Backtesting


**In-Sample testing**

To evaluate the performance of our predictive models, we begin by conducting in-sample (IS) testing. This involves fitting each model on a fixed training sample and evaluating how well the model explains historical variation in the data.

We assess in-sample performance using the following criteria:

\begin{itemize}
  \item \textbf{Mean Squared Error (MSE)}: Measures the average squared difference between predicted and actual returns.
  \[
  \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2
  \]
  
  \item \textbf{Adjusted \( R^2 \)}: Indicates the proportion of variance explained by the model, adjusted for the number of predictors.
  \[
  R_{\text{adj}}^2 = 1 - \frac{\text{RSS}/(n - p - 1)}{\text{TSS}/(n - 1)}
  \]

  \item \textbf{Directional Accuracy}: The fraction of times the predicted direction matches the actual direction of returns.
  \[
  \text{Accuracy} = \frac{1}{n} \sum_{i=1}^{n} \mathbb{1} \left( \text{sign}(\hat{y}_i) = \text{sign}(y_i) \right)
  \]
\end{itemize}

These metrics are computed for all three models:
\begin{enumerate}
  \item Benchmark (7-day momentum only),
  \item Extended linear model with multiple features,
  \item Lasso-regularized regression with automatic feature selection.
\end{enumerate}

```{r, echo = FALSE}
# --- Benchmark model (already estimated) ---
model_benchmark <- lm(target_return ~ momentum_7d, data = df_extended)

# --- Predictions ---
df_extended <- df_extended %>%
  mutate(
    pred_benchmark = predict(model_benchmark),
    pred_extended = predict(model_extended),
    pred_lasso = as.numeric(predict(model_lasso, newx = X)),

    # Directional accuracy
    dir_true = sign(target_return),
    dir_benchmark = sign(pred_benchmark),
    dir_extended = sign(pred_extended),
    dir_lasso = sign(pred_lasso)
  )

# --- Evaluation metrics ---
mse <- function(pred, actual) mean((pred - actual)^2)
acc <- function(pred, actual) mean(sign(pred) == sign(actual))

results_is <- tibble(
  Model = c("Benchmark", "Extended", "Lasso"),
  MSE = c(
    mse(df_extended$pred_benchmark, df_extended$target_return),
    mse(df_extended$pred_extended, df_extended$target_return),
    mse(df_extended$pred_lasso, df_extended$target_return)
  ),
  Directional_Accuracy = c(
    acc(df_extended$pred_benchmark, df_extended$target_return),
    acc(df_extended$pred_extended, df_extended$target_return),
    acc(df_extended$pred_lasso, df_extended$target_return)
  ),
  Adj_R2 = c(
    summary(model_benchmark)$adj.r.squared,
    summary(model_extended)$adj.r.squared,
    NA  # glmnet doesn't provide adj. R^2
  )
)

print(results_is)

```

#TODO: interpret results



**Out-of-sample testing:**

#TODO: review code, does not work at the moment 

#TODO: 
Evaluate:
o Sharpe ratio
o Cumulative return
o OOSR2
o Hit rate (how often you correctly predict direction)


```{r, echo = FALSE}
# Rolling OOS function (for any linear model formula)
# run_oos_forecast <- function(df, model_formula, window_size = 500) {
#   n <- nrow(df)
#   preds <- rep(NA, n)
#   actuals <- rep(NA, n)
#   mean_forecast <- rep(NA, n)
# 
#   for (i in (window_size + 1):(n - 1)) {
#     train_data <- df[(i - window_size):(i - 1), ]
#     test_data <- df[i, ]
# 
#     model <- lm(model_formula, data = train_data)
#     preds[i + 1] <- predict(model, newdata = test_data)
#     actuals[i + 1] <- df$target_return[i + 1]
#     mean_forecast[i + 1] <- mean(train_data$target_return, na.rm = TRUE)
#   }
# 
#   tibble(
#     time = df$date,
#     forecast = preds,
#     actual = actuals,
#     mean_forecast = mean_forecast
#   ) %>% drop_na()
# }
# 
# # Run for benchmark model
# oos_benchmark <- run_oos_forecast(df_extended, target_return ~ momentum_7d)
# 
# # Compute R2_OS
# r2_os <- 1 - sum((oos_benchmark$actual - oos_benchmark$forecast)^2) /
#              sum((oos_benchmark$actual - oos_benchmark$mean_forecast)^2)
# cat("OOS R^2 (Benchmark):", round(r2_os, 4), "\n")
# 
# # Plot CSPE
# cspe_df <- oos_benchmark %>%
#   mutate(
#     cspe_model = cumsum((actual - forecast)^2),
#     cspe_mean = cumsum((actual - mean_forecast)^2)
#   )
# 
# ggplot(cspe_df, aes(x = time)) +
#   geom_line(aes(y = cspe_mean, color = "Historical Mean")) +
#   geom_line(aes(y = cspe_model, color = "Model Forecast")) +
#   labs(title = "Cumulative Squared Prediction Error (CSPE)",
#        x = "Date", y = "CSPE") +
#   scale_color_manual(values = c("Historical Mean" = "black", "Model Forecast" = "blue"))

```

```{r, echo = FALSE}

#define a function to run the rolling out-of-sample lasso regression
run_rolling_lasso <- function(df, y_var, window_size = 500, x_vars = NULL) {
  # Drop rows where the target variable is NA
  df <- df %>% filter(!is.na(.data[[y_var]]))
  
  # Automatically detect predictors if not provided
  if (is.null(x_vars)) {
    predictors <- setdiff(names(df), c("date", y_var))
  } else {
    predictors <- x_vars
  }
  
  # Storage for predictions
  predictions <- rep(NA, nrow(df))

  # Rolling window loop
  for (i in seq(window_size + 1, nrow(df))) {
    # Get training and test data
    train_data <- df[(i - window_size):(i - 1), ]
    test_data <- df[i, ]

    # Extract X and y matrices
    x_train <- as.matrix(train_data[, predictors])
    y_train <- train_data[[y_var]]

    x_test <- as.matrix(test_data[, predictors])

    # Fit LASSO model
    cv_fit <- cv.glmnet(x_train, y_train, alpha = 1)
    best_lambda <- cv_fit$lambda.min
    # Refit LASSO model using best lambda
    model <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda)

   # Predict for one-step ahead
    predictions[i] <- predict(model, newx = x_test)
  }

  # Return tibble with actual vs predicted
  tibble(
    date = df$date,
    actual = df[[y_var]],
    predicted = predictions
  )
}

x_vars <- colnames(df_extended)[!(colnames(df_extended) %in% c("date", "target_return"))]

# Keep only numeric predictors
x_vars_numeric <- x_vars[sapply(df_extended[, x_vars], is.numeric)]

# Filter for complete cases in x_vars_numeric + target_return
df_clean <- df_extended %>%
  dplyr::select(date, target_return, all_of(x_vars_numeric)) %>%
  filter(complete.cases(.))

#For each out-of-sample prediction, the model is trained on the previous 500 observations
results <- run_rolling_lasso(
  df = df_clean,
  y_var = "target_return",
  window_size = 500,
  x_vars = colnames(df_clean)[!(colnames(df_clean) %in% c("date", "target_return"))]
)

```



```{r, echo = FALSE}
# Make sure 'results' has no NAs in actual or predicted
results_clean <- results %>% filter(!is.na(actual), !is.na(predicted))

# Compute performance metrics
r_squared <- cor(results_clean$actual, results_clean$predicted)^2
mse <- mean((results_clean$actual - results_clean$predicted)^2)
directional_accuracy <- mean(sign(results_clean$actual) == sign(results_clean$predicted))

# Compute Sharpe Ratios (daily returns assumed)
sharpe_ratio_actual <- mean(results_clean$actual) / sd(results_clean$actual) * sqrt(252)
sharpe_ratio_pred <- mean(results_clean$predicted) / sd(results_clean$predicted) * sqrt(252)

# Print metrics
performance_metrics <- tibble(
  `R-squared` = r_squared,
  `MSE` = mse,
  `Directional Accuracy` = directional_accuracy,
  `Sharpe Ratio (Actual)` = sharpe_ratio_actual,
  `Sharpe Ratio (Predicted)` = sharpe_ratio_pred
)

performance_metrics %>%
  kable("latex", booktabs = TRUE, digits = 4,
        caption = "Table 5: Out-of-Sample Performance Metrics of LASSO Model") %>%
  kable_styling(latex_options = c("striped", "hold_position", "scale_down")) %>%
  row_spec(0, bold = TRUE)


```
```{r, echo = FALSE}
# Plot: Predicted vs Actual
# ggplot(results_clean, aes(x = actual, y = predicted)) +
#   geom_point(alpha = 0.4, color = "steelblue") +
#   geom_smooth(method = "lm", se = FALSE, color = "darkred") +
#   labs(
#     title = "Predicted vs. Actual Returns",
#     x = "Actual Returns",
#     y = "Predicted Returns"
#   ) +
#   theme_minimal()
```


add transaction fees as extra path 

```{r, echo = FALSE}

#Performance Comparison Momentum 7-day and buy-hold

#Cumulative returns
df_daily <- df_daily %>%
  mutate(
    cum_ret_strategy = cumsum(coalesce(strategy_return, 0)),
    cum_ret_bh = cumsum(coalesce(log_return, 0))
  )

# Plot
ggplot(df_daily, aes(x = date)) +
  geom_line(aes(y = cum_ret_strategy, color = "Strategy")) +
  geom_line(aes(y = cum_ret_bh, color = "Buy & Hold")) +
  labs(title = "7-Day Momentum Strategy vs Buy-and-Hold",
       x = "Date", y = "Cumulative Log Return") +
  scale_color_manual(values = c("Strategy" = "blue", "Buy & Hold" = "black"))
```

```{r, echo = FALSE}

df_extended$lasso_pred <- results$predicted

df_extended <- df_extended %>%
  mutate(
    lasso_signal = ifelse(lasso_pred > 0, 1, -1),  # buy/sell signal
    lasso_strategy_return = lasso_signal * target_return  # realized return based on prediction
  )

df_extended <- df_extended %>%
  mutate(
    cum_ret_lasso = cumsum(coalesce(lasso_strategy_return, 0)),
    cum_ret_momentum = cumsum(coalesce(strategy_return, 0)),  # from your existing momentum signal
    cum_ret_bh = cumsum(coalesce(log_return, 0))  # buy-and-hold
  )

library(dplyr)
library(ggplot2)

ggplot(df_extended, aes(x = date)) +
  geom_line(aes(y = cum_ret_momentum, color = "7-Day Momentum")) +
  geom_line(aes(y = cum_ret_bh, color = "Buy & Hold")) +
  geom_line(aes(y = cum_ret_lasso, color = "LASSO Strategy")) +
  labs(title = "Cumulative Log Returns: LASSO vs Momentum vs Buy & Hold",
       x = "Date", y = "Cumulative Log Return") +
  scale_color_manual(values = c("7-Day Momentum" = "blue",
                                "Buy & Hold" = "black",
                                "LASSO Strategy" = "darkred")) +
  theme_minimal()

```


## 6. Conclusion



